\begin{enumerate}
    \item{\mathbf{CSCE 676 – Data Mining and Analysis (Dr. James Caverlee)}  
    \begin{itemize}
       \item Generated association rules (support >=minsup, confidence >= minconf) using hash-based Apriori to generate the rules)
        \item Ranked the graph nodes using the Personal Page Ranking 
        \item Performed Text Mining, Sentimental Analysis, Topic Modelling, Expectation Maximization on millions of text file crawled from Google regarding corporate social responsibilities 
        \item Understood MapReduce programming model; Studied the concepts in Hadoop; Analyzed the tweets concerning members of the US congress (such as most mentioned users) on AWS using Spark with Pregel 
        \item Performed K-means for different clusters; Hierarchical clustering (single link & complete link) 
        \item Tufte's Principles (such as maximizing the data-ink ratio & erasing redundant data-ink)
        \item Finding similarities using Jaccard distance with shingling, min-hashing, and locality-sensitive hashing (LSH) 
        \item Understood consistent hashing on Napster; Studied structured P2P approaches such as Chord using Finger table; Used counting bloom filter algorithm to test whether a movie is a member of a set;  
        \item Used Reservoir sampling in data stream; Adopted Morris algorithm and Flajolet-Martin algorithm to perform probabilistic counting 
        \item Studied Privacy-Preserving data mining for association rule mining, k-means clustering, and decision trees.  
        \item Used K-anonymity to remove personally identifying information; Understood differential privacy;  
 
    \end{itemize}
    
  \item{\mathbf{CSCE 633 – Machine Learning (Dr. Bobak Mortazavi) }  
    \begin{itemize}
       \item Studied multiple linear regression with model fitting; Understood the concept of convexity;   \item Studied multiple logistic regression and perform optimization using cross-entropy (gradient descent with different learning rate) 
       \item Performed model selection with lasso(L2) and ridge(L1) regression  
       \item Studied SVM using hyperplane to classify the data; several techniques are: 1) maximal marginal hyperplane; 2) Enlarge through kernel space; 3) Adopted Hinge Loss and Lagrange Multipliers 
       \item Studied decision tree and used Gini Index to avoid overfitting; Studied random forests and AdaBoost  
       \item Used PCA to reduce the dimension and K-Means to find patterns in clusters
       \item Studied Expectation Maximization to increase the likelihood   
       \item Studied neural network with the concepts of perceptron, forward/backward propagation calculation. \item Learnt how convolutional neural networks use kernels and max-pooling techniques.  
       \item Understood reinforcement learning and its Hidden Markov Model 
       \item Predicted NASDAQ price with LSTM (RNN) using Keras API and archived 75% up/down prediction accuracy. (Hourly data from 2009 to 2019) 
    \end{itemize}
    
    
  \item{\mathbf{ CSCE 636 Deep Learning (Dr. Shuiwang Ji)   
       \begin{itemize}
       \item Studied the concept of Perceptron, threshold and Perceptron Learning Algorithm (PLA).   
       \item Studied linear classification, Pocket Algorithm, Least Squares Linear Regression. 
       \item Adopted Cross Entropy and maximized the likelihood to minimize the total error using Stochastic Gradient Descent.  
       \item Calculated probability for multi-class logistic regression using softmax; Measured the error using corss entropy to calculate individual class’s loss; Studied the shift-invariance in parameters (output remain the same); Studied the equivalence of sigmoid when cross entropy cope with binary classes;
       \item Calculated the derivative of software and derivative of cross entropy with softmax;
       \item Learnt different loss functions such as: Hinge Loss, Log Loss, Zero-one Loss; Studied convexity;  \item Studied how to use regularization (Lasso, Ridge), cross-validation and early stopping techniques to prevent overfitting during model selection;  
       \item Studied the multilayer perceptron (MLP) and the reason to use adopt tanh(.) activation function (original error is not smooth and cannot use gradient descent) 
       \item Utilized Chain Rule to produce sensitivities during the backpropagation process. 
       \item Compared different descents such as steepest descents, conjugate gradients, etc. 
       \item Understood thoroughly about SVD and its relation to Eigen-Decomposition; Learnt compact SVD; Learnt how to compute PCA and the reconstruction process; Studied Autoencoder; 
       \item Studied the concept of fully connected layer, filters, max-pooling, ReLU, stride and padding in CNN and how to calculate the output volume size;   
       \item Studied batch normalization and the suitable position to insert in CNN; Advantages such as improving gradient flow through the network, allowing higher learning rate and reducing the strong dependence on initialization;  
       \item Understood SGD with Momentum, second-order optimization; Understood model ensembles, dropout, data augmentation to prevent overfiting.
       \item Understood the concept of transfer learning;  
       \item Learnt different CNN such as AlexN;
       \item Calculated the loss in RNN Language Model and performed the backpropagation with multivariable chain rule;  
       \item Understood the vanishing and exploding gradient problem in RNN; Studied the architecture of LSTM; \item Understood the mechanism in Attention;  
       \item Knowledge in Graph Convolutional  Networks; Knowledge in graph pooling with clustering and its limitations;
    \end{itemize}}
\end{enumerate}
